{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide extend of jupyter\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import Image, display, HTML\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='progress')\n",
    "pd.set_option('max_colwidth',200)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FILE_PATH = '/home/eiao/pythoncode/FaceReco'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_file_path = os.path.join(FILE_PATH, 'datasets/data.csv')\n",
    "data_df = pd.read_csv(label_file_path)\n",
    "data_df['image'] = data_df['image'].apply(lambda x: os.path.join(FILE_PATH, 'dataset/image', x))\n",
    "data_df = data_df.sample(frac=0.01, replace=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from models.cnn import mini_XCEPTION\n",
    "from utils.datasets import DataManager\n",
    "from utils.datasets import split_data\n",
    "from utils.preprocessor import preprocess_input\n",
    "\n",
    "# parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10000\n",
    "input_shape = (64, 64, 1)\n",
    "validation_split = .2\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "base_path = '../trained_models/emotion_models/'\n",
    "\n",
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "# model parameters/compilation\n",
    "model = mini_XCEPTION(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "datasets = ['fer2013']\n",
    "for dataset_name in datasets:\n",
    "    print('Training dataset:', dataset_name)\n",
    "\n",
    "    # callbacks\n",
    "    log_file_path = base_path + dataset_name + '_emotion_training.log'\n",
    "    csv_logger = CSVLogger(log_file_path, append=False)\n",
    "    early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
    "                                  patience=int(patience/4), verbose=1)\n",
    "    trained_models_path = base_path + dataset_name + '_mini_XCEPTION'\n",
    "    model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
    "                                                    save_best_only=True)\n",
    "    callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "    # loading dataset\n",
    "    data_loader = DataManager(dataset_name, image_size=input_shape[:2])\n",
    "    faces, emotions = data_loader.get_data()\n",
    "    faces = preprocess_input(faces)\n",
    "    num_samples, num_classes = emotions.shape\n",
    "    train_data, val_data = split_data(faces, emotions, validation_split)\n",
    "    train_faces, train_emotions = train_data\n",
    "    model.fit_generator(data_generator.flow(train_faces, train_emotions,\n",
    "                                            batch_size),\n",
    "                        steps_per_epoch=len(train_faces) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data,test_data,train_targets,test_targets = train_test_split(data_df['image'].tolist(),data_df['label'].tolist(),test_size=0.2)\n",
    "\n",
    "# data loader\n",
    "image_train_dataset = ImageDataset(train_data, train_targets, transform=image_train_transform)\n",
    "image_test_dataset = ImageDataset(test_data, test_targets, transform=image_test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(image_train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(image_test_dataset, batch_size=1, shuffle=False)\n",
    "# train_loader = torch.utils.data.DataLoader(image_train_dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)\n",
    "# test_loader = torch.utils.data.DataLoader(image_test_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnext_101_64x4d import resnext_101_64x4d\n",
    "from models.model import ResnextLogo\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "IMG_MODEL_PATH = os.path.join(FILE_PATH, 'trained_model')   # save trained model at this path\n",
    "\n",
    "model = resnext_101_64x4d\n",
    "model.load_state_dict(torch.load(os.path.join(FILE_PATH, 'models/resnext_101_64x4d.pth')))\n",
    "model = ResnextLogo(model)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.require_grad = True\n",
    "for param in list(model.children())[0].parameters():\n",
    "    param.require_grad = False\n",
    "# model.cuda()\n",
    "model_name = '_'.join([model.__class__.__name__.lower(), '101', '64*4d', str(int(time.time()))])\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "model.train()\n",
    "\n",
    "from utils.train import train, validate, adjust_learning_rate, save_checkpoint\n",
    "epochs = 70\n",
    "model_version = \"_\".join([model_name, 'small_test', 'Adam', 'lr: 0.001_0.01'])\n",
    "\n",
    "# trian parameters:  \n",
    "layer_resnext_train_parameters = list(model.children())[1].parameters()\n",
    "layer_classifier_parameters = list(model.children())[2].parameters()\n",
    "\n",
    "#choose lr: \n",
    "# optimizer = torch.optim.RMSprop([{\"params\":layerfc_parameters,\"lr\":0.01},{\"params\":layer4_parameters,\"lr\":0.001},{\"params\":layer3_parameters,\"lr\":1e-4},{\"params\":base_params,\"lr\":1e-4}], momentum=0.9, weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam([{\"params\":layer_resnext_train_parameters,\"lr\":0.001},{\"params\":layer_classifier_parameters,\"lr\":0.01}])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "best_prec1 = 0\n",
    "loss_epoch = []\n",
    "metrics = list()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "#     adjust_learning_rate(optimizer, epoch, base_lr)\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write('-' * 20)\n",
    "    sys.stdout.write('\\n* Epoch {}\\n'.format(epoch))\n",
    "\n",
    "    # train for one epoch\n",
    "    train_prec1, train_precn, train_loss = train(train_loader, model, criterion, optimizer, top_num=3)\n",
    "    print()\n",
    "    # evaluate on validation set\n",
    "    val_prec1, val_precn, val_loss = validate(test_loader, model, criterion, top_num=3)\n",
    "\n",
    "    sys.stdout.write('\\n* Prec@1 {val_prec1:.3f} {now}\\n'.format(val_prec1=val_prec1, now=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    metrics.append(dict(\n",
    "        epoch=epoch,\n",
    "        train_prec1 = train_prec1,\n",
    "        train_precn = train_precn,\n",
    "        train_loss = train_loss,\n",
    "        val_prec1 = val_prec1,\n",
    "        val_precn = val_precn,\n",
    "        val_loss = val_loss\n",
    "    ))\n",
    "\n",
    "    is_best = val_prec1 > best_prec1\n",
    "    best_prec1 = max(val_prec1, best_prec1)\n",
    "    save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': model_name,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'model_version': model_version,\n",
    "            'version': model_version,\n",
    "            'metrics': metrics,\n",
    "        }, 'best' if is_best else 'normal', IMG_MODEL_PATH, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "import matplotlib.pyplot as plt\n",
    "metrics_df[['train_prec1', 'val_prec1']].plot(xlim=(0, len(metrics_df)), ylim=(0, 100))\n",
    "metrics_df[['train_loss', 'val_loss']].plot(xlim=(0, len(metrics_df)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "import matplotlib.pyplot as plt\n",
    "metrics_df[['train_prec1', 'val_prec1']].plot(xlim=(0, len(metrics_df)), ylim=(0, 100))\n",
    "metrics_df[['train_loss', 'val_loss']].plot(xlim=(0, len(metrics_df)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_df.loc[metrics_df['val_prec1'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
