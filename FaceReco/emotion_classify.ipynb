{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wide extend of jupyter\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import Image, display, HTML\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc='progress')\n",
    "pd.set_option('max_colwidth',200)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FILE_PATH = '/home/eiao/pythoncode/StudyShare/FaceReco'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704 176 220\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_emotions = 3   # number of out emotion\n",
    "img_size = 256\n",
    "\n",
    "def dataLoad(img_list, label_list, num_emotions=3, height=28, width=28, V2=True):\n",
    "    # image: path to array\n",
    "    # label: str to array\n",
    "    num = len(img_list)\n",
    "    all_img = np.zeros((num, height, width, 1))\n",
    "    all_label = np.zeros((num, num_emotions))\n",
    "    for i in range(num):\n",
    "        img = cv2.imread(img_list[i], 0)\n",
    "        img = cv2.resize(img, (height,width)).astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        if V2:\n",
    "            img = (img - 0.5) * 2.0\n",
    "        all_img[i,:,:,0] = img\n",
    "        all_label[i,:] = np.asarray((label_list[i])[2:-1].split(','))\n",
    "    return all_img, all_label\n",
    "\n",
    "\n",
    "label_file_path = os.path.join(FILE_PATH, 'datasets/data.csv')\n",
    "data_df = pd.read_csv(label_file_path)\n",
    "data_df['image'] = data_df['image'].apply(lambda x: os.path.join(FILE_PATH, 'datasets/image', x))\n",
    "# data_df = data_df.sample(frac=0.1, replace=True)\n",
    "\n",
    "train_data,test_data,train_targets,test_targets = train_test_split(data_df['image'].values,data_df['labels'].values,test_size=0.2)\n",
    "train_data,val_data,train_targets,val_targets = train_test_split(train_data,train_targets,test_size=0.2)\n",
    "print(len(train_targets), len(val_targets), len(test_targets))\n",
    "\n",
    "\n",
    "train_x, train_y = dataLoad(train_data, train_targets, num_emotions=num_emotions, height=img_size, width=img_size, V2=True)\n",
    "val_x, val_y = dataLoad(val_data, val_targets, num_emotions=num_emotions, height=img_size, width=img_size, V2=True)\n",
    "test_x, test_y = dataLoad(test_data, test_targets, num_emotions=num_emotions, height=img_size, width=img_size, V2=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 254, 254, 8)  72          input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 254, 254, 8)  32          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 254, 254, 8)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 252, 252, 8)  576         activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 252, 252, 8)  32          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 252, 252, 8)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_41 (SeparableC (None, 252, 252, 16) 200         activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 252, 252, 16) 64          separable_conv2d_41[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 252, 252, 16) 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_42 (SeparableC (None, 252, 252, 16) 400         activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 252, 252, 16) 64          separable_conv2d_42[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 126, 126, 16) 128         activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 126, 126, 16) 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 126, 126, 16) 64          conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 126, 126, 16) 0           max_pooling2d_21[0][0]           \n",
      "                                                                 batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_43 (SeparableC (None, 126, 126, 32) 656         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 126, 126, 32) 128         separable_conv2d_43[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 126, 126, 32) 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_44 (SeparableC (None, 126, 126, 32) 1312        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 126, 126, 32) 128         separable_conv2d_44[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 63, 63, 32)   512         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 63, 63, 32)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 63, 63, 32)   128         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 63, 63, 32)   0           max_pooling2d_22[0][0]           \n",
      "                                                                 batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_45 (SeparableC (None, 63, 63, 64)   2336        add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 63, 63, 64)   256         separable_conv2d_45[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 63, 63, 64)   0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_46 (SeparableC (None, 63, 63, 64)   4672        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 63, 63, 64)   256         separable_conv2d_46[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 32, 32, 64)   2048        add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 32, 32, 64)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 64)   256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 32, 32, 64)   0           max_pooling2d_23[0][0]           \n",
      "                                                                 batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_47 (SeparableC (None, 32, 32, 128)  8768        add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_47[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 32, 128)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_48 (SeparableC (None, 32, 32, 128)  17536       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 32, 32, 128)  512         separable_conv2d_48[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 128)  8192        add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 16, 16, 128)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 128)  512         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 16, 16, 128)  0           max_pooling2d_24[0][0]           \n",
      "                                                                 batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 3)    3459        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 3)            0           conv2d_42[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 53,811\n",
      "Trainable params: 52,339\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 4/44 [=>............................] - ETA: 19:15 - loss: 1.6810"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b82ad924be76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     validation_data=(val_x, val_y))\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from models.test_model import mini_XCEPTION\n",
    "\n",
    "# parameters\n",
    "batch_size = 4\n",
    "num_epochs = 3\n",
    "input_shape = (img_size, img_size, 1)\n",
    "verbose = 1\n",
    "\n",
    "patience = 50\n",
    "base_path = os.path.join(FILE_PATH, 'trained_models')\n",
    "\n",
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=False)\n",
    "\n",
    "# model parameters/compilation\n",
    "model = mini_XCEPTION(input_shape, num_emotions)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# callbacks\n",
    "log_file_path = os.path.join(base_path,'_emotion_training.log')\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "model_names = os.path.join(base_path, '_mini_XCEPTION_{epoch:02d}.hdf5')\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1, save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "# model fit\n",
    "\n",
    "model.fit_generator(data_generator.flow(train_x, train_y,\n",
    "                                        batch_size),\n",
    "                    steps_per_epoch=len(train_x) / batch_size,\n",
    "                    epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                    validation_data=(val_x, val_y))\n",
    "\n",
    "score = model.evaluate(test_x, test_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 18s 83ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9041195583614436"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(test_x, test_y, batch_size=batch_size)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 15s 83ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97677121383392"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(val_x, val_y, batch_size=batch_size)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29249957, 1.4951167 , 1.832237  ],\n",
       "       [0.30739826, 1.4942718 , 1.803678  ],\n",
       "       [0.30609393, 1.4594591 , 1.820226  ],\n",
       "       [0.29795006, 1.4776753 , 1.8156567 ],\n",
       "       [0.39889458, 1.8176898 , 1.6204807 ],\n",
       "       [0.32584167, 1.5457534 , 1.4278563 ],\n",
       "       [0.28940213, 1.4523591 , 1.800651  ],\n",
       "       [0.29447654, 1.4615823 , 1.6195652 ],\n",
       "       [0.2953256 , 1.5471742 , 1.7212932 ],\n",
       "       [0.30626008, 1.4746546 , 1.8252991 ],\n",
       "       [0.26063794, 1.3984848 , 1.6250513 ],\n",
       "       [0.3032226 , 1.4556172 , 1.823037  ],\n",
       "       [0.34001714, 1.5332137 , 1.6312652 ],\n",
       "       [0.2946477 , 1.4426903 , 1.7863046 ],\n",
       "       [0.30355117, 1.4411235 , 1.8116202 ],\n",
       "       [0.4021537 , 1.8429725 , 1.315437  ],\n",
       "       [0.347598  , 1.6664913 , 1.7122136 ],\n",
       "       [0.27078268, 1.454285  , 1.6658734 ],\n",
       "       [0.3998894 , 1.7062552 , 1.4721189 ],\n",
       "       [0.3583171 , 1.6282601 , 1.5994477 ],\n",
       "       [0.2930316 , 1.4091312 , 1.8046381 ],\n",
       "       [0.3000719 , 1.5029023 , 1.8160722 ],\n",
       "       [0.2984587 , 1.4473015 , 1.8223511 ],\n",
       "       [0.29343122, 1.4648027 , 1.8251599 ],\n",
       "       [0.28493968, 1.4998842 , 1.6106284 ],\n",
       "       [0.47365108, 1.8204442 , 1.4803184 ],\n",
       "       [0.41160637, 1.7385792 , 1.5856282 ],\n",
       "       [0.29064175, 1.4648015 , 1.8413525 ],\n",
       "       [0.5844223 , 2.232027  , 1.8049985 ],\n",
       "       [0.30999696, 1.4580859 , 1.7931819 ],\n",
       "       [0.31902   , 1.4936604 , 1.777978  ],\n",
       "       [0.35539213, 1.6553761 , 1.6672997 ],\n",
       "       [0.51088375, 2.013132  , 1.4021553 ],\n",
       "       [0.30010533, 1.4649755 , 1.8157976 ],\n",
       "       [0.44891864, 1.7603945 , 1.3876448 ],\n",
       "       [0.3015684 , 1.4670614 , 1.7777271 ],\n",
       "       [0.28915247, 1.4664845 , 1.8288401 ],\n",
       "       [0.30387542, 1.4634218 , 1.8251467 ],\n",
       "       [0.323967  , 1.5090995 , 1.8095001 ],\n",
       "       [0.29843703, 1.4128699 , 1.6965975 ],\n",
       "       [0.43779856, 1.9166353 , 1.4772737 ],\n",
       "       [0.2641214 , 1.5663499 , 1.5276387 ],\n",
       "       [0.29425606, 1.4348671 , 1.8212875 ],\n",
       "       [0.29175267, 1.4254727 , 1.8199179 ],\n",
       "       [0.3119333 , 1.4800888 , 1.6715342 ],\n",
       "       [0.29606467, 1.6992025 , 1.5982449 ],\n",
       "       [0.27681327, 1.4071175 , 1.7954208 ],\n",
       "       [0.32433513, 1.5014963 , 1.7699006 ],\n",
       "       [0.2600275 , 1.4653653 , 1.6139389 ],\n",
       "       [0.30047688, 1.4518725 , 1.8125362 ],\n",
       "       [0.31018203, 1.4751981 , 1.7739965 ],\n",
       "       [0.2937707 , 1.4675369 , 1.828169  ],\n",
       "       [0.3000083 , 1.4489214 , 1.8178813 ],\n",
       "       [0.46971062, 1.8345731 , 1.4108986 ],\n",
       "       [0.3187378 , 1.4420223 , 1.7793164 ],\n",
       "       [0.3298386 , 1.5296483 , 1.6445582 ],\n",
       "       [0.2807225 , 1.5230564 , 1.7574506 ],\n",
       "       [0.6267871 , 2.3055596 , 1.4607351 ],\n",
       "       [0.328297  , 1.4854223 , 1.6540395 ],\n",
       "       [0.28188503, 1.439012  , 1.6664184 ],\n",
       "       [0.3926015 , 1.6839304 , 1.4813243 ],\n",
       "       [0.34729788, 1.6276816 , 1.766393  ],\n",
       "       [0.28733438, 1.4847863 , 1.7715136 ],\n",
       "       [0.29508382, 1.432059  , 1.8205848 ],\n",
       "       [0.31751052, 1.7551371 , 1.4676204 ],\n",
       "       [0.28229377, 1.5234522 , 1.7114091 ],\n",
       "       [0.464982  , 1.8115438 , 1.3469805 ],\n",
       "       [0.29048917, 1.4473562 , 1.8288898 ],\n",
       "       [0.30103883, 1.4357107 , 1.6214442 ],\n",
       "       [0.3063422 , 1.4431027 , 1.8062109 ],\n",
       "       [0.30266485, 1.4554553 , 1.8212426 ],\n",
       "       [0.29919508, 1.5099804 , 1.6464485 ],\n",
       "       [0.3043255 , 1.4552662 , 1.8120037 ],\n",
       "       [0.35148183, 1.5591377 , 1.7308837 ],\n",
       "       [0.26291937, 1.4801215 , 1.5976238 ],\n",
       "       [0.48062158, 1.8627352 , 1.5393555 ],\n",
       "       [0.3015289 , 1.4818256 , 1.7435865 ],\n",
       "       [0.26523244, 1.5123035 , 1.5897083 ],\n",
       "       [0.3857287 , 1.5573831 , 1.5536132 ],\n",
       "       [0.29829144, 1.4379443 , 1.5306917 ],\n",
       "       [0.3972971 , 1.6852251 , 1.4263176 ],\n",
       "       [0.27503702, 1.5359513 , 1.7441362 ],\n",
       "       [0.2855481 , 1.4374232 , 1.812393  ],\n",
       "       [0.24080585, 1.3893015 , 1.6695423 ],\n",
       "       [0.29020655, 1.4271355 , 1.8019959 ],\n",
       "       [0.3047609 , 1.4858483 , 1.8304555 ],\n",
       "       [0.3282346 , 1.5570835 , 1.6330711 ],\n",
       "       [0.27395582, 1.6976035 , 1.5844976 ],\n",
       "       [0.52041864, 1.9739313 , 1.6905619 ],\n",
       "       [0.24991459, 1.4362211 , 1.7982843 ],\n",
       "       [0.2982972 , 1.4841025 , 1.814814  ],\n",
       "       [0.4207209 , 1.5560336 , 1.4587393 ],\n",
       "       [0.3570778 , 1.8047338 , 1.42666   ],\n",
       "       [0.3097951 , 1.6536549 , 1.5852941 ],\n",
       "       [0.66301495, 2.306969  , 1.7856567 ],\n",
       "       [0.34055248, 1.7410042 , 1.3802245 ],\n",
       "       [0.3601699 , 1.5728295 , 1.7547529 ],\n",
       "       [0.30511698, 1.4837772 , 1.8199881 ],\n",
       "       [0.2918839 , 1.4226682 , 1.7890856 ],\n",
       "       [0.27033716, 1.4688767 , 1.8133497 ],\n",
       "       [0.29958495, 1.6622534 , 1.5691361 ],\n",
       "       [0.33219105, 1.5700258 , 1.5847539 ],\n",
       "       [0.2724989 , 1.5520725 , 1.6497855 ],\n",
       "       [0.3260418 , 1.4698865 , 1.8138975 ],\n",
       "       [0.39703107, 1.8603187 , 1.5880685 ],\n",
       "       [0.27900854, 1.5081357 , 1.8088961 ],\n",
       "       [0.3524739 , 1.6966108 , 1.8039804 ],\n",
       "       [0.30952206, 1.5656548 , 1.7484934 ],\n",
       "       [0.294747  , 1.6419439 , 1.4978051 ],\n",
       "       [0.41378975, 1.7741201 , 1.7250947 ],\n",
       "       [0.3084634 , 1.4415997 , 1.7573923 ],\n",
       "       [0.30217227, 1.4958519 , 1.7594795 ],\n",
       "       [0.32494038, 1.4429369 , 1.6804351 ],\n",
       "       [0.25236562, 1.3827237 , 1.4827596 ],\n",
       "       [0.3080318 , 1.5339861 , 1.693692  ],\n",
       "       [0.30749303, 1.4845216 , 1.5508776 ],\n",
       "       [0.31132564, 1.4569533 , 1.4269028 ],\n",
       "       [0.2972676 , 1.4589517 , 1.8153828 ],\n",
       "       [0.4016035 , 1.7000524 , 1.5263295 ],\n",
       "       [0.30629602, 1.4612782 , 1.8208114 ],\n",
       "       [0.3747014 , 1.646078  , 1.7928784 ],\n",
       "       [0.40045923, 1.6231973 , 1.5064824 ],\n",
       "       [0.29730192, 1.4378855 , 1.815361  ],\n",
       "       [0.28087887, 1.4387712 , 1.7696146 ],\n",
       "       [0.33500364, 1.4846323 , 1.5419422 ],\n",
       "       [0.30362204, 1.4450556 , 1.8179634 ],\n",
       "       [0.27478126, 1.4921026 , 1.7875644 ],\n",
       "       [0.28679892, 1.4407789 , 1.8359151 ],\n",
       "       [0.29588553, 1.4131957 , 1.7858438 ],\n",
       "       [0.27905872, 1.3655169 , 1.3745588 ],\n",
       "       [0.31290784, 1.5755173 , 1.742527  ],\n",
       "       [0.41931772, 1.7839466 , 1.5123563 ],\n",
       "       [0.2897165 , 1.480071  , 1.8006182 ],\n",
       "       [0.33499837, 1.6347847 , 1.7021717 ],\n",
       "       [0.45280752, 1.7292068 , 1.6216758 ],\n",
       "       [0.2970155 , 1.4984549 , 1.8022397 ],\n",
       "       [0.2859246 , 1.4463373 , 1.7705778 ],\n",
       "       [0.3345804 , 1.5173527 , 1.627013  ],\n",
       "       [0.32291356, 1.5331993 , 1.692051  ],\n",
       "       [0.26116607, 1.4028171 , 1.7786065 ],\n",
       "       [0.2779356 , 1.5201172 , 1.6314783 ],\n",
       "       [0.28925508, 1.481054  , 1.8240663 ],\n",
       "       [0.3542195 , 1.598624  , 1.6534407 ],\n",
       "       [0.3763225 , 1.6989096 , 1.809713  ],\n",
       "       [0.29596204, 1.4420936 , 1.8250895 ],\n",
       "       [0.27453542, 1.450525  , 1.8058711 ],\n",
       "       [0.37219775, 1.7369509 , 1.4947283 ],\n",
       "       [0.33802137, 1.6285753 , 1.807618  ],\n",
       "       [0.32662246, 1.5387079 , 1.7655872 ],\n",
       "       [0.29834256, 1.4452837 , 1.8277283 ],\n",
       "       [0.3289154 , 1.4697458 , 1.5477915 ],\n",
       "       [0.3211944 , 1.4506384 , 1.6447749 ],\n",
       "       [0.2984436 , 1.4712404 , 1.74349   ],\n",
       "       [0.31084335, 1.4803776 , 1.806491  ],\n",
       "       [0.2791871 , 1.4553043 , 1.7469329 ],\n",
       "       [0.34976327, 1.6588936 , 1.6874738 ],\n",
       "       [0.28401133, 1.4303224 , 1.795073  ],\n",
       "       [0.4114535 , 1.7160408 , 1.2842298 ],\n",
       "       [0.28265136, 1.7816086 , 1.5764863 ],\n",
       "       [0.30354282, 1.5364866 , 1.5129116 ],\n",
       "       [0.33816007, 1.6350797 , 1.54275   ],\n",
       "       [0.57590383, 2.1577146 , 1.4803728 ],\n",
       "       [0.3095247 , 1.463078  , 1.8190085 ],\n",
       "       [0.39045557, 1.6243112 , 1.428187  ],\n",
       "       [0.2777227 , 1.6315721 , 1.5381984 ],\n",
       "       [0.28079614, 1.6390342 , 1.5807298 ],\n",
       "       [0.30373818, 1.5576706 , 1.6607915 ],\n",
       "       [0.3046101 , 1.4535248 , 1.8187244 ],\n",
       "       [0.3139539 , 1.5767264 , 1.5536486 ],\n",
       "       [0.29119882, 1.491826  , 1.7729875 ],\n",
       "       [0.29900402, 1.4797883 , 1.702677  ],\n",
       "       [0.29871187, 1.4752102 , 1.7394145 ],\n",
       "       [0.29483452, 1.5885584 , 1.7456164 ],\n",
       "       [0.327017  , 1.5571493 , 1.7633272 ],\n",
       "       [0.2984086 , 1.5998595 , 1.6166117 ],\n",
       "       [0.5675917 , 2.1993873 , 1.5133775 ],\n",
       "       [0.31204244, 1.449351  , 1.7218616 ],\n",
       "       [0.2836495 , 1.4024667 , 1.5778883 ],\n",
       "       [0.2707258 , 1.5586414 , 1.7266831 ],\n",
       "       [0.32932827, 1.479164  , 1.7786771 ],\n",
       "       [0.46026585, 1.9689945 , 1.607652  ],\n",
       "       [0.27138728, 1.4885285 , 1.6868107 ],\n",
       "       [0.34606287, 1.9174213 , 1.6579884 ],\n",
       "       [0.30779293, 1.4829891 , 1.6864567 ],\n",
       "       [0.305313  , 1.4579282 , 1.7165465 ],\n",
       "       [0.29393196, 1.5103211 , 1.617307  ],\n",
       "       [0.292955  , 1.4542274 , 1.7969509 ],\n",
       "       [0.28870603, 1.5608119 , 1.8040905 ],\n",
       "       [0.52633405, 2.002667  , 1.7687578 ],\n",
       "       [0.46768132, 1.9161854 , 1.7320468 ],\n",
       "       [0.2758081 , 1.559408  , 1.5380027 ],\n",
       "       [0.3313926 , 1.5698217 , 1.6845438 ],\n",
       "       [0.30229267, 1.4572716 , 1.8229047 ],\n",
       "       [0.33538678, 1.4970318 , 1.5703961 ],\n",
       "       [0.31557888, 1.4447715 , 1.4580177 ],\n",
       "       [0.36035773, 1.780771  , 1.6885123 ],\n",
       "       [0.2945137 , 1.4619144 , 1.8341627 ],\n",
       "       [0.26847413, 1.4283462 , 1.7452925 ],\n",
       "       [0.30698985, 1.472556  , 1.7922679 ],\n",
       "       [0.27565092, 1.4357213 , 1.7917378 ],\n",
       "       [0.3148425 , 1.5393394 , 1.7723056 ],\n",
       "       [0.31281555, 1.4993875 , 1.420778  ],\n",
       "       [0.50959074, 1.9429722 , 1.5641956 ],\n",
       "       [0.34132126, 1.6628038 , 1.4651308 ],\n",
       "       [0.27882278, 1.5791032 , 1.4745218 ],\n",
       "       [0.4161396 , 1.794006  , 1.5507166 ],\n",
       "       [0.3212266 , 1.5262051 , 1.6267173 ],\n",
       "       [0.35734564, 1.6185489 , 1.5139768 ],\n",
       "       [0.2931471 , 1.5906065 , 1.4559647 ],\n",
       "       [0.269553  , 1.4433943 , 1.7489578 ],\n",
       "       [0.26744124, 1.4877175 , 1.7650925 ],\n",
       "       [0.2694651 , 1.4225416 , 1.5778705 ],\n",
       "       [0.28599283, 1.4598907 , 1.7756822 ],\n",
       "       [0.26483685, 1.4599776 , 1.8284212 ],\n",
       "       [0.42051318, 1.7559446 , 1.4867275 ],\n",
       "       [0.29839143, 1.4267521 , 1.8176372 ],\n",
       "       [0.32727775, 1.545613  , 1.6708556 ],\n",
       "       [0.31092095, 1.4704851 , 1.8142147 ],\n",
       "       [0.29475018, 1.4374392 , 1.665502  ],\n",
       "       [0.3423447 , 1.6628822 , 1.4873725 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = model.predict(test_x, batch_size=batch_size)\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_smile</th>\n",
       "      <th>predict_sad</th>\n",
       "      <th>prrdict_angry</th>\n",
       "      <th>real_smile</th>\n",
       "      <th>real_sad</th>\n",
       "      <th>real_angry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.292500</td>\n",
       "      <td>1.495117</td>\n",
       "      <td>1.832237</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.307398</td>\n",
       "      <td>1.494272</td>\n",
       "      <td>1.803678</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.306094</td>\n",
       "      <td>1.459459</td>\n",
       "      <td>1.820226</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.297950</td>\n",
       "      <td>1.477675</td>\n",
       "      <td>1.815657</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.398895</td>\n",
       "      <td>1.817690</td>\n",
       "      <td>1.620481</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.325842</td>\n",
       "      <td>1.545753</td>\n",
       "      <td>1.427856</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.289402</td>\n",
       "      <td>1.452359</td>\n",
       "      <td>1.800651</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.294477</td>\n",
       "      <td>1.461582</td>\n",
       "      <td>1.619565</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.295326</td>\n",
       "      <td>1.547174</td>\n",
       "      <td>1.721293</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.306260</td>\n",
       "      <td>1.474655</td>\n",
       "      <td>1.825299</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.260638</td>\n",
       "      <td>1.398485</td>\n",
       "      <td>1.625051</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.303223</td>\n",
       "      <td>1.455617</td>\n",
       "      <td>1.823037</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.340017</td>\n",
       "      <td>1.533214</td>\n",
       "      <td>1.631265</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.294648</td>\n",
       "      <td>1.442690</td>\n",
       "      <td>1.786305</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.303551</td>\n",
       "      <td>1.441123</td>\n",
       "      <td>1.811620</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.402154</td>\n",
       "      <td>1.842973</td>\n",
       "      <td>1.315437</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.347598</td>\n",
       "      <td>1.666491</td>\n",
       "      <td>1.712214</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.270783</td>\n",
       "      <td>1.454285</td>\n",
       "      <td>1.665873</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.20</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.399889</td>\n",
       "      <td>1.706255</td>\n",
       "      <td>1.472119</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.358317</td>\n",
       "      <td>1.628260</td>\n",
       "      <td>1.599448</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.293032</td>\n",
       "      <td>1.409131</td>\n",
       "      <td>1.804638</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.300072</td>\n",
       "      <td>1.502902</td>\n",
       "      <td>1.816072</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.298459</td>\n",
       "      <td>1.447302</td>\n",
       "      <td>1.822351</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.293431</td>\n",
       "      <td>1.464803</td>\n",
       "      <td>1.825160</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.284940</td>\n",
       "      <td>1.499884</td>\n",
       "      <td>1.610628</td>\n",
       "      <td>0.20</td>\n",
       "      <td>4.20</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.473651</td>\n",
       "      <td>1.820444</td>\n",
       "      <td>1.480318</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.411606</td>\n",
       "      <td>1.738579</td>\n",
       "      <td>1.585628</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.290642</td>\n",
       "      <td>1.464802</td>\n",
       "      <td>1.841352</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.584422</td>\n",
       "      <td>2.232027</td>\n",
       "      <td>1.804999</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.309997</td>\n",
       "      <td>1.458086</td>\n",
       "      <td>1.793182</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.275808</td>\n",
       "      <td>1.559408</td>\n",
       "      <td>1.538003</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.331393</td>\n",
       "      <td>1.569822</td>\n",
       "      <td>1.684544</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.302293</td>\n",
       "      <td>1.457272</td>\n",
       "      <td>1.822905</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.335387</td>\n",
       "      <td>1.497032</td>\n",
       "      <td>1.570396</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.315579</td>\n",
       "      <td>1.444772</td>\n",
       "      <td>1.458018</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.360358</td>\n",
       "      <td>1.780771</td>\n",
       "      <td>1.688512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.294514</td>\n",
       "      <td>1.461914</td>\n",
       "      <td>1.834163</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.268474</td>\n",
       "      <td>1.428346</td>\n",
       "      <td>1.745293</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.306990</td>\n",
       "      <td>1.472556</td>\n",
       "      <td>1.792268</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.275651</td>\n",
       "      <td>1.435721</td>\n",
       "      <td>1.791738</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.314842</td>\n",
       "      <td>1.539339</td>\n",
       "      <td>1.772306</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.312816</td>\n",
       "      <td>1.499388</td>\n",
       "      <td>1.420778</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.509591</td>\n",
       "      <td>1.942972</td>\n",
       "      <td>1.564196</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.341321</td>\n",
       "      <td>1.662804</td>\n",
       "      <td>1.465131</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.278823</td>\n",
       "      <td>1.579103</td>\n",
       "      <td>1.474522</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.416140</td>\n",
       "      <td>1.794006</td>\n",
       "      <td>1.550717</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.321227</td>\n",
       "      <td>1.526205</td>\n",
       "      <td>1.626717</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.357346</td>\n",
       "      <td>1.618549</td>\n",
       "      <td>1.513977</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.293147</td>\n",
       "      <td>1.590606</td>\n",
       "      <td>1.455965</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.269553</td>\n",
       "      <td>1.443394</td>\n",
       "      <td>1.748958</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.60</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.267441</td>\n",
       "      <td>1.487718</td>\n",
       "      <td>1.765092</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.269465</td>\n",
       "      <td>1.422542</td>\n",
       "      <td>1.577870</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.285993</td>\n",
       "      <td>1.459891</td>\n",
       "      <td>1.775682</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.264837</td>\n",
       "      <td>1.459978</td>\n",
       "      <td>1.828421</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.420513</td>\n",
       "      <td>1.755945</td>\n",
       "      <td>1.486727</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.298391</td>\n",
       "      <td>1.426752</td>\n",
       "      <td>1.817637</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.327278</td>\n",
       "      <td>1.545613</td>\n",
       "      <td>1.670856</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.310921</td>\n",
       "      <td>1.470485</td>\n",
       "      <td>1.814215</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.294750</td>\n",
       "      <td>1.437439</td>\n",
       "      <td>1.665502</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.342345</td>\n",
       "      <td>1.662882</td>\n",
       "      <td>1.487373</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predict_smile  predict_sad  prrdict_angry  real_smile  real_sad  \\\n",
       "0         0.292500     1.495117       1.832237        0.80      0.20   \n",
       "1         0.307398     1.494272       1.803678        0.00      0.60   \n",
       "2         0.306094     1.459459       1.820226        0.40      1.00   \n",
       "3         0.297950     1.477675       1.815657        0.00      3.20   \n",
       "4         0.398895     1.817690       1.620481        0.25      0.00   \n",
       "5         0.325842     1.545753       1.427856        0.60      1.60   \n",
       "6         0.289402     1.452359       1.800651        0.00      5.00   \n",
       "7         0.294477     1.461582       1.619565        0.60      2.20   \n",
       "8         0.295326     1.547174       1.721293        0.75      0.00   \n",
       "9         0.306260     1.474655       1.825299        0.50      0.50   \n",
       "10        0.260638     1.398485       1.625051        0.60      0.00   \n",
       "11        0.303223     1.455617       1.823037        0.00      1.50   \n",
       "12        0.340017     1.533214       1.631265        0.80      2.60   \n",
       "13        0.294648     1.442690       1.786305        0.00      0.80   \n",
       "14        0.303551     1.441123       1.811620        0.60      1.20   \n",
       "15        0.402154     1.842973       1.315437        0.60      1.40   \n",
       "16        0.347598     1.666491       1.712214        0.20      2.20   \n",
       "17        0.270783     1.454285       1.665873        0.40      2.20   \n",
       "18        0.399889     1.706255       1.472119        0.60      1.40   \n",
       "19        0.358317     1.628260       1.599448        0.25      2.50   \n",
       "20        0.293032     1.409131       1.804638        0.80      1.00   \n",
       "21        0.300072     1.502902       1.816072        0.00      2.75   \n",
       "22        0.298459     1.447302       1.822351        0.40      2.80   \n",
       "23        0.293431     1.464803       1.825160        0.00      2.00   \n",
       "24        0.284940     1.499884       1.610628        0.20      4.20   \n",
       "25        0.473651     1.820444       1.480318        0.25      0.00   \n",
       "26        0.411606     1.738579       1.585628        0.40      2.20   \n",
       "27        0.290642     1.464802       1.841352        0.00      2.00   \n",
       "28        0.584422     2.232027       1.804999        0.80      3.00   \n",
       "29        0.309997     1.458086       1.793182        0.00      0.40   \n",
       "..             ...          ...            ...         ...       ...   \n",
       "190       0.275808     1.559408       1.538003        0.20      1.00   \n",
       "191       0.331393     1.569822       1.684544        0.60      2.40   \n",
       "192       0.302293     1.457272       1.822905        0.80      2.40   \n",
       "193       0.335387     1.497032       1.570396        0.40      1.00   \n",
       "194       0.315579     1.444772       1.458018        0.60      2.20   \n",
       "195       0.360358     1.780771       1.688512        0.00      1.00   \n",
       "196       0.294514     1.461914       1.834163        0.50      1.25   \n",
       "197       0.268474     1.428346       1.745293        0.00      1.20   \n",
       "198       0.306990     1.472556       1.792268        0.80      1.80   \n",
       "199       0.275651     1.435721       1.791738        0.40      0.80   \n",
       "200       0.314842     1.539339       1.772306        0.00      0.80   \n",
       "201       0.312816     1.499388       1.420778        0.40      0.80   \n",
       "202       0.509591     1.942972       1.564196        0.60      1.40   \n",
       "203       0.341321     1.662804       1.465131        0.20      1.00   \n",
       "204       0.278823     1.579103       1.474522        0.00      1.20   \n",
       "205       0.416140     1.794006       1.550717        0.00      0.20   \n",
       "206       0.321227     1.526205       1.626717        0.80      3.20   \n",
       "207       0.357346     1.618549       1.513977        0.00      2.60   \n",
       "208       0.293147     1.590606       1.455965        0.20      1.00   \n",
       "209       0.269553     1.443394       1.748958        0.20      2.60   \n",
       "210       0.267441     1.487718       1.765092        0.80      0.40   \n",
       "211       0.269465     1.422542       1.577870        0.80      1.20   \n",
       "212       0.285993     1.459891       1.775682        0.00      0.20   \n",
       "213       0.264837     1.459978       1.828421        0.60      1.00   \n",
       "214       0.420513     1.755945       1.486727        0.00      0.50   \n",
       "215       0.298391     1.426752       1.817637        0.80      1.00   \n",
       "216       0.327278     1.545613       1.670856        0.40      0.60   \n",
       "217       0.310921     1.470485       1.814215        0.00      1.40   \n",
       "218       0.294750     1.437439       1.665502        0.20      0.80   \n",
       "219       0.342345     1.662882       1.487373        0.80      0.20   \n",
       "\n",
       "     real_angry  \n",
       "0          0.20  \n",
       "1          0.80  \n",
       "2          1.80  \n",
       "3          1.60  \n",
       "4          0.00  \n",
       "5          0.40  \n",
       "6          2.20  \n",
       "7          2.80  \n",
       "8          0.00  \n",
       "9          0.00  \n",
       "10         0.00  \n",
       "11         3.75  \n",
       "12         1.00  \n",
       "13         3.80  \n",
       "14         1.00  \n",
       "15         1.00  \n",
       "16         3.40  \n",
       "17         4.00  \n",
       "18         1.00  \n",
       "19         1.00  \n",
       "20         0.60  \n",
       "21         1.50  \n",
       "22         3.60  \n",
       "23         3.00  \n",
       "24         2.00  \n",
       "25         0.00  \n",
       "26         3.40  \n",
       "27         1.25  \n",
       "28         3.00  \n",
       "29         0.60  \n",
       "..          ...  \n",
       "190        1.00  \n",
       "191        1.60  \n",
       "192        4.60  \n",
       "193        1.00  \n",
       "194        3.40  \n",
       "195        3.20  \n",
       "196        1.25  \n",
       "197        3.20  \n",
       "198        3.40  \n",
       "199        2.80  \n",
       "200        1.20  \n",
       "201        0.00  \n",
       "202        3.00  \n",
       "203        0.60  \n",
       "204        0.20  \n",
       "205        0.20  \n",
       "206        4.00  \n",
       "207        1.00  \n",
       "208        2.40  \n",
       "209        4.40  \n",
       "210        0.20  \n",
       "211        1.00  \n",
       "212        1.40  \n",
       "213        2.80  \n",
       "214        0.25  \n",
       "215        1.00  \n",
       "216        1.00  \n",
       "217        1.00  \n",
       "218        0.40  \n",
       "219        0.20  \n",
       "\n",
       "[220 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre = map(lambda x: x[0], emotions)\n",
    "pre = pd.DataFrame(emotions,columns=['predict_smile', 'predict_sad', 'prrdict_angry'])\n",
    "real = pd.DataFrame(test_y, columns=['real_smile', 'real_sad', 'real_angry'])\n",
    "result = pd.concat([pre, real], axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models.resnext_101_64x4d import resnext_101_64x4d\n",
    "from models.model import ResnextLogo\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "IMG_MODEL_PATH = os.path.join(FILE_PATH, 'trained_model')   # save trained model at this path\n",
    "\n",
    "model = resnext_101_64x4d\n",
    "model.load_state_dict(torch.load(os.path.join(FILE_PATH, 'models/resnext_101_64x4d.pth')))\n",
    "model = ResnextLogo(model)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.require_grad = True\n",
    "for param in list(model.children())[0].parameters():\n",
    "    param.require_grad = False\n",
    "# model.cuda()\n",
    "model_name = '_'.join([model.__class__.__name__.lower(), '101', '64*4d', str(int(time.time()))])\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "import matplotlib.pyplot as plt\n",
    "metrics_df[['train_prec1', 'val_prec1']].plot(xlim=(0, len(metrics_df)), ylim=(0, 100))\n",
    "metrics_df[['train_loss', 'val_loss']].plot(xlim=(0, len(metrics_df)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics)\n",
    "import matplotlib.pyplot as plt\n",
    "metrics_df[['train_prec1', 'val_prec1']].plot(xlim=(0, len(metrics_df)), ylim=(0, 100))\n",
    "metrics_df[['train_loss', 'val_loss']].plot(xlim=(0, len(metrics_df)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_df.loc[metrics_df['val_prec1'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
